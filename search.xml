<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>zero</title>
    <url>/2021/06/25/index/</url>
    <content><![CDATA[<h2 id="Welcome-to-My-Channel"><a href="#Welcome-to-My-Channel" class="headerlink" title="Welcome to My Channel"></a>Welcome to My Channel</h2><p>My name is Klein, and I am currently working and studying in <strong>financial</strong> sector in Shanghai. Having a <strong>statistical</strong> academic background, I want to use this channel to share my personal insights as well as some small projects with you guys. </p>
<p>Below I have attached several links to my other pages. Feel free to take a tour if you are interested in those topics. (Remember to access those links only at home page. Otherwise, the links won’t work as in the post.)</p>
<div class="menu">
    <ul>
      <br>
    <a href="cn">我的中文页面</a>
      </br>
          <br>
    <a href="academics">Academic Interests</a>
      </br>
            <br>
    <a href="journals">Scholarly Journals</a>
            </br>
            <br>
    <a href="news2021">Newsroom</a>
            </br>
    </ul>
</div>

<p>In my personal columns at <a href="https://www.zhihu.com/people/wang-yuan-chen-24/columns">Zhihu</a>, I have been posting weekly articles about financial news and reading notes. I will publish my personal <a href="https://zhuanlan.zhihu.com/p/366324411">reading list</a> every 6 months, in which I recommend 20 of my favorite books to you. </p>
]]></content>
  </entry>
  <entry>
    <title>Body Reform | 健身日志01</title>
    <url>/2021/05/11/%E5%81%A5%E8%BA%AB%E6%97%A5%E5%BF%9701/</url>
    <content><![CDATA[<blockquote>
<p>Train like a Spartan.</p>
</blockquote>
<p>第一次尝试以文字的形式来记录下自己日常健身时的训练内容，希望可以不断改善自己的训练计划，找到最适合自己的生活方式。</p>
<p>运动期间我每日的热量大概在1800千卡左右，宏观营养摄入量如下：</p>
<ul>
<li>脂肪 37-45g</li>
<li>蛋白质 104-127g</li>
<li>碳水 229-280g</li>
</ul>
<p>这次的日志记录了4月末为期10天（4/21-4/30）的训练内容，平均下来一周每个肌群会训练1到2次。由于我自己的体脂含量偏高，所以力量训练结束后会进行30min左右的有氧，来加速身体内脂肪的燃烧。</p>
<img src="/2021/05/11/%E5%81%A5%E8%BA%AB%E6%97%A5%E5%BF%9701/1.jpg" class="">

<img src="/2021/05/11/%E5%81%A5%E8%BA%AB%E6%97%A5%E5%BF%9701/2.jpg" class="">

<img src="/2021/05/11/%E5%81%A5%E8%BA%AB%E6%97%A5%E5%BF%9701/3.jpg" class="">

<p>运动带给我们的好处非常之多，无论背后的目标是减脂、增肌、身体塑性或是长久形成的生活习惯，都可以借由社交平台一起来探讨训练内外的各种心得和知识。之后的内容会继续从饮食、睡眠、训练、心态等等的角度来记录健身日常，喜欢的小伙伴欢迎点赞支持一下哟，一起自律打卡。</p>
]]></content>
      <categories>
        <category>hobbies</category>
      </categories>
      <tags>
        <tag>training</tag>
      </tags>
  </entry>
  <entry>
    <title>Hindsight Experience Replay</title>
    <url>/2021/06/21/p1/</url>
    <content><![CDATA[<p><em><strong>Hindsight Experience Replay</strong></em></p>
<p>Authors: Marcin Andrychowicz∗ , Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel† , Wojciech Zaremba†</p>
<p>Link: <a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1707.01495">https://arxiv.org/abs/1707.01495</a></p>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><blockquote>
<p>Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task. The video presenting our experiments is available at <a href="https://link.zhihu.com/?target=https://goo.gl/SMrQnI">https://goo.gl/SMrQnI</a>.</p>
</blockquote>
<h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>HER (Hindsight Experience Replay) is introduced to allow the algorithm to learn almost as much from achieving an undesired outcome as from the desired one, as humans do. Unlike the current generation of model-free RL algorithms, HER makes the learning possible even if the reward signal is unshaped (i.e. sparse and binary). </p>
<p>In many cased of reinforcement learning, we often need to augment the reward using domain knowledge, in what is known as Reward Engineering or Reward Shaping. The problem here is that it is not always practically workable to discover the proper shaping for the reward functions. In other words, the domain knowledge required for strengthening learning is not always available. </p>
<h4 id="Example-amp-Methodology"><a href="#Example-amp-Methodology" class="headerlink" title="Example &amp; Methodology"></a>Example &amp; Methodology</h4><p>In this paper, the authors have given out a motivating example, which asks us to consider a bit-flipping environment (page3) with state space <img src="https://www.zhihu.com/equation?tex=S=(0,1)%5En"> , action space <img src="https://www.zhihu.com/equation?tex=A=%7B0,1,...,n-1%7D">for some integer n in which executing the i-th action flips the i-th bit of the state. The policy gets a reward of -1 as long as it is not in the target state, i.e. <img src="https://www.zhihu.com/equation?tex=r_g(s,a)=-%5Bs%5Cne+g%5D"> . </p>
<p>As mentioned in the example, a standard solution to this problem would be to use a shaped reward function which is more informative and guides the agent towards the goal, e.g. <img src="https://www.zhihu.com/equation?tex=r_g(s,a)=-%7C%7Cs-g%7C%7C%5E2"> . But this approach may be difficult to apply to more complicated problems (i.e. hard to capture the full information using shaped reward function like above). </p>
<p>The second approach that has been raised is HER, which the reasoning is explained below:</p>
<blockquote>
<p>The pivotal idea behind our approach is to re-examine this trajectory with a different goal — while this trajectory may not help us learn how to achieve the state <img src="https://www.zhihu.com/equation?tex=g"> , it definitely tells us something about how to achieve the state <img src="https://www.zhihu.com/equation?tex=s_T">.</p>
</blockquote>
<p><strong>Off-Policy Learning</strong></p>
<p>But how do human deal with such problem using HER? Sometimes when we fail to perform some tasks, we recognize that what we have done could be useful in another context, or for another task. It is the intuition that the authors of the paper used to develop their method. </p>
<p>In HER, the authors suggest the following strategy: suppose our agent performs an episode of trying to reach goal state G from initial state S, but fails to do so and ends up in some state S’ at the end of the episode. We cache the trajectory into our replay buffer where <img src="https://www.zhihu.com/equation?tex=r_k">is the reward received at step k of the episode, and<img src="https://www.zhihu.com/equation?tex=a_k">is the action taken at step k of the episode.</p>
<p>The idea in HER is to <strong>imagine that our goal has actually been S’ all along</strong>, and that in this alternative reality our agent has reached the goal successfully and got the positive reward for doing so.</p>
<p>In addition to caching the real trajectory, we also cache the trajectory with imagined goal S’. This trajectory is motivated by the human ability to learn useful things from failed attempts. By introducing the imagined trajectories to our replay buffer, we ensure that<strong>no matter how bad our policy is, it will always have some positive rewards to learn from</strong>.</p>
<p>The magic of function approximation by neural networks will ensure that our policy could also reach states similar to those it has seen before; this is the generalization property that is the hallmark of successful deep learning. At first, the agent will be able to reach states in a relatively small area around the initial state, but gradually it expands this reachable area of the state space until finally it learns to reach those goal states we are actually interested in.</p>
<p><img src="https://pic1.zhimg.com/80/v2-ff722eecbf0f8f8ef16586d10451d288_1440w.jpg" alt="Pseudo Code of HER"></p>
<blockquote>
<p>HER may be seen as a form of implicit curriculum as the goals used for replay naturally shift from ones which are simple to achieve even by a random agent to more difficult ones. However, in contrast to explicit curriculum, HER does not require having any control over the distribution of initial environment states.</p>
</blockquote>
]]></content>
      <categories>
        <category>academics</category>
        <category>journal review</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Credit Scoring via Logistic Regression</title>
    <url>/2021/06/23/p3/</url>
    <content><![CDATA[<p><em><strong>Credit Scoring via Logistic Regression</strong></em></p>
<p>Author: Ali Al-Arad</p>
<p>Link: <a href="https://link.zhihu.com/?target=http://utstat.toronto.edu/~ali/papers/creditworthinessProject.pdf">http://utstat.toronto.edu/~ali/papers/creditworthinessProject.pdf</a></p>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><blockquote>
<p>The goal of credit scoring models is to predict the creditworthiness of a customer and determine whether they will be able to meet a given financial obligation or default on it. Such models allow a financial institution to minimize the risk of loss by setting decision rules regarding which customers receive loan and credit card approvals. Logistic regression can be used to predict default events and model the influence of different variables on a consumer’s creditworthiness. In this paper we use a logistic regression model to predict the creditworthiness of bank customers using predictors related to their personal status and financial history. Model adequacy and robustness checks are performed to ensure that the model is being properly fitted and interpreted.</p>
</blockquote>
<h4 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h4><p>The data set used is the German Credit dataset obtained from the UCI machine-learning data archive and includes 20 covariates (7 numerical, 13 categorical) and 1000 observations. Each observation represents an individual customer with the response indicating their actual classification (1 = “Good” or 0 = “Bad”). </p>
<p>For the purpose of this paper, 5 major predictors have been selected to build on the logistic model. The predictors cover financial, living and social indicators. </p>
<p><img src="https://pic3.zhimg.com/80/v2-c3989f3d6ce654878902a3a1f6605c06_1440w.jpg"></p>
<h4 id="Introduction-amp-Modelling"><a href="#Introduction-amp-Modelling" class="headerlink" title="Introduction &amp; Modelling"></a>Introduction &amp; Modelling</h4><p>In this paper, a binary logistic model is fitted to the data, using the logit link function. In other words, the classification of the <img src="https://www.zhihu.com/equation?tex=i_%7Bth%7D"> customer as having good or bad creditability is modeled using a Bernoulli random variable:</p>
<p><img src="https://www.zhihu.com/equation?tex=Y_i=%5Cleft%5C%7B+%5Cbegin%7Baligned%7D+1+&+,+&+%5Cmbox%7Bif+the+customer+is+creditworthy%7D+%5C%5C+0+&+,+&+%5Cmbox%7Botherwise%7D+%5Cend%7Baligned%7D+%5Cright."></p>
<p>with conditional probabilities <img src="https://www.zhihu.com/equation?tex=P(Y_i=1%7Cx_i)=%5Cpi_i"> and <img src="https://www.zhihu.com/equation?tex=P(Y_i=0%7Cx_i)=1-%5Cpi_i">. </p>
<p>The conditional expectation is the given by: <img src="https://www.zhihu.com/equation?tex=E(Y_i%7Cx_i)=P(Y_i=1%7Cx_i)%5Ctimes+1+P(Y_i=0%7Cx_i)%5Ctimes+0=%5Cpi_i"> . </p>
<p>The link function is essentially transferring the predicted probability into a more interpretable indicator which in this case is odds ratio: </p>
<p>logit <img src="https://www.zhihu.com/equation?tex=%5Cpi_i=%5Clog(%5Cfrac%7B%5Cpi_i%7D%7B1-%5Cpi_i%7D)=x_i%5E%7B%27%7D%5Cbeta=%5Ceta_i"></p>
<p>The estimation is performed by iterative weighted least squares (IWLS). </p>
<h4 id="Model-Adequacy"><a href="#Model-Adequacy" class="headerlink" title="Model Adequacy"></a>Model Adequacy</h4><p>In terms of model validation, the author first tested on the significance of the deviance reduction, which suggest weak evidence against the model/link given large p-value. Since the Pearson chi-square statistic or the deviance likelihood ratio test are not informative, Hosmer Lemeshow Test was performed to further compare the test statistic of observations in g categories to a <img src="https://www.zhihu.com/equation?tex=%5Cchi%5E2_%7Bg-2%7D"> distribution. Similar result was given.</p>
<h4 id="Outlier-detection"><a href="#Outlier-detection" class="headerlink" title="Outlier detection"></a>Outlier detection</h4><p>In this paper, Cook’s Distance was used to plot the observations and identified three outliers in the data set. Comparing original model with a second model by removing those three cases to study their impact on the estimation and conclusions, the author has found that removal of those cases does not lead to noticeable changes in estimated parameters. </p>
<h4 id="Robustness-Checks"><a href="#Robustness-Checks" class="headerlink" title="Robustness Checks"></a>Robustness Checks</h4><p>An alternative model is fit using aggregated data, to see if the conclusions of these models agree with those of the original model. By grouping the covariate data prior to aggregation, a smaller range of covariate patterns (i.e. collinearity) would be achieved. In particular, the categories of 4 factor variables have been more general, including history, duration, marital status and purpose. Based on Likelihood ratio test, the results agree with those of the original model. The exceptions are that when gender and status are considered separately, they are not significant variables, and purpose is also insignificant at 95% significance level. </p>
<p><img src="https://pic4.zhimg.com/80/v2-0330cb83f1e1dd922a9435e1d9f06ab7_1440w.jpg"></p>
<h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>The main findings of statistical analysis are as follows:</p>
<ul>
<li>Odds of a consumer’s creditworthiness increase with an increase in the size of their checking account.</li>
<li>Odds of a consumer’s creditworthiness is 1.636 times greater when he is a single male than who being divorced/married females. </li>
<li>Odds of a consumer’s creditworthiness with a purpose of car is likely to be the greatest.</li>
<li>Increased duration decreases the odds of creditworthiness.</li>
<li>Consumers with “critical” credit history show large increases in expected odds of creditworthiness. In other words, the result suggests that consumers with worse credit history are less likely to default. It is very important that author has given out two possible explanations:</li>
</ul>
<ol>
<li>The bank may be more stringent when it comes to loaning a consumer with bad credit history, whereas consumers with good credit history do not face the same kind of scrutiny and may end up being issued a loan they eventually cannot repay.</li>
<li>An alternative explanation is that there may be a data issue in which the categories were incorrectly labeled. It would be best to be cautious with the interpretation of this result.</li>
</ol>
]]></content>
      <categories>
        <category>academics</category>
        <category>journal review</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
        <tag>Finance</tag>
      </tags>
  </entry>
  <entry>
    <title>PlanGAN, Model-based Planning With Sparse Rewards and Multiple Goals</title>
    <url>/2021/06/22/p2/</url>
    <content><![CDATA[<p><em><strong>PlanGAN: Model-based Planning With Sparse Rewards and Multiple Goals</strong></em></p>
<p>Authors: Henry Charlesworth, Giovanni Montana</p>
<p>Link: <a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2006.00900">https://arxiv.org/abs/2006.00900</a></p>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><blockquote>
<p>Learning with sparse rewards remains a significant challenge in reinforcement learning (RL), especially when the aim is to train a policy capable of achieving multiple different goals. To date, the most successful approaches for dealing with multi-goal, sparse reward environments have been model-free RL algorithms. In this work we propose PlanGAN, a model-based algorithm specifically designed for solving multi-goal tasks in environments with sparse rewards. Our method builds on the fact that any trajectory of experience collected by an agent contains useful information about how to achieve the goals observed during that trajectory. We use this to train an ensemble of conditional generative models (GANs) to generate plausible trajectories that lead the agent from its current state towards a specified goal. We then combine these imagined trajectories into a novel planning algorithm in order to achieve the desired goal as efficiently as possible. The performance of PlanGAN has been tested on a number of robotic navigation/manipulation tasks in comparison with a range of model-free reinforcement learning baselines, including Hindsight Experience Replay. Our studies indicate that PlanGAN can achieve comparable performance whilst being around 4-8 times more sample efficient.</p>
</blockquote>
<h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>In this paper the authors present PlanGAN, a model-based algorithm that can naturally be applied to sparse reward environments with multiple goals. The core of this method builds upon the same principle that underlies HER — namely that any goal observed during a given trajectory can be used as an example of how to achieve that goal from states that occurred earlier on in that same trajectory. </p>
<p>However, unlike HER, the algorithm does not directly learn a goal-conditioned policy/value function but rather train an ensemble of Generative Adversarial Networks (GANs) which learn to generate plausible future trajectories conditioned on achieving a particular goal. Then these imagined trajectories are combined into a novel planning algorithm that can reach those goals in an efficient manner.</p>
<h4 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h4><p>The aim of the first major component of the methodology is to train a generative model that can take in the current state <img src="https://www.zhihu.com/equation?tex=s_t"> along with a desired goal <img src="https://www.zhihu.com/equation?tex=g"> and produce an imagined action at and next state <img src="https://www.zhihu.com/equation?tex=s_%7Bt+1%7D"> that moves the agent towards achieving <img src="https://www.zhihu.com/equation?tex=g"> .</p>
<p>Intuitionally, the authors want to use the model to take a state-action pair <img src="https://www.zhihu.com/equation?tex=(s_t,+a_t)"> and predict the difference between the next state and current state, <img src="https://www.zhihu.com/equation?tex=s_%7Bt+1%7D-s_t"> . The predictive models in the paper are used to provide an L2 regularisation (Ridge) term in the generator loss that encourages the generated actions and next states to be consistent with the predictions of the one-step models. </p>
<p><img src="https://pic3.zhimg.com/80/v2-e428832189cfaff49e0b58d32a048efe_1440w.jpg"></p>
<p>The basic structure of the planner is to make use of a model to generate a number of imaginary future trajectories, score them, use these scores to choose the next action, and repeat this whole procedure at the next step. The score captures how effective those trajectories are in terms of moving towards the final goal <img src="https://www.zhihu.com/equation?tex=g"> . A good score here should reflect the fact that we want the next action to be moving us towards <img src="https://www.zhihu.com/equation?tex=g"> as quickly as possible whilst also ensuring that the goal can be retained at later time steps.</p>
<p>Once these trajectories have been generated, the researchers give each of them a score based on the <em>fraction of time they spend achieving the goal</em>. This means that trajectories that reach the goal quickly are scored highly, but only if they are able to remain there. Accordingly, trajectories that do not reach the goal within T steps are given a score of zero. They can then score each of the initial actions <img src="https://www.zhihu.com/equation?tex=(a%5Eq_t)%5EQ_%7Bq=1%7D"> based on the average score of all the imagined trajectories that started with that action. These scores are normalised and denoted as <img src="https://www.zhihu.com/equation?tex=n_i"> . The final action returned by the planner is either the action with the maximum score or an exponentially weighted average of the initially proposed actions, <img src="https://www.zhihu.com/equation?tex=a_t=%5Cfrac%7B%5Csum_%7Bi=1%7D%5E%7BR%7D%7Be%5E%7B%5Calpha+n_i%7D%5Calpha_i%7D%7D%7B%5Csum_%7Bj=1%7D%5E%7BQ%7D%7Be%5E%7B%5Calpha+n_i%7D%7D%7D"> , where <img src="https://www.zhihu.com/equation?tex=%5Calpha%3E0"> is a hyperparameter. </p>
<p><img src="https://pic1.zhimg.com/80/v2-f6288903a114299b9de194b129be7598_1440w.jpg"></p>
]]></content>
      <categories>
        <category>academics</category>
        <category>journal review</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>读书笔记《存在主义咖啡馆》</title>
    <url>/2021/06/30/%E5%AD%98%E5%9C%A8%E4%B8%BB%E4%B9%89%E5%92%96%E5%95%A1%E9%A6%86/</url>
    <content><![CDATA[<p>副标题：自由、存在和杏子鸡尾酒</p>
<p>原作名：At the Existentialist Café: Freedom, Being, and Apricot Cocktails</p>
<p>译者：沈敏一</p>
<img src="/2021/06/30/%E5%AD%98%E5%9C%A8%E4%B8%BB%E4%B9%89%E5%92%96%E5%95%A1%E9%A6%86/1.jpg" class="">

<h2 id="三句话来概述"><a href="#三句话来概述" class="headerlink" title="三句话来概述"></a>三句话来概述</h2><ol>
<li>莎拉·贝克韦尔将历史、传记与哲学结合在一起，以史诗般恢弘的视角，激情地讲述了一个充满了斗争、爱情、反抗与背叛的存在主义故事，深入探讨了在今天这个纷争不断、技术驱动的世界里，当我们每个人再次面对有关绝对自由、全球责任与人类真实性的问题时，曾经也受过它们困扰的存在主义者能告诉我们什么。</li>
<li>在书里哲学被塑造成了一种人生模式，通过人物传记的方式哲学家们的个人经验和他们的哲学思考形成了精神上的融合。</li>
<li>别想着向外求索；返回你自身。真理栖居于灵魂之中。（圣奥古斯丁）</li>
</ol>
<h2 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h2><p>莎拉·贝克韦尔（Sarah Bakewell），1963年出生于英国的伯恩茅斯，后随父母在亚洲旅行多年，最终在澳大利亚悉尼定居、长大。返回英国后，她考入埃塞克斯大学，攻读哲学专业，毕业后在伦敦的一家图书馆做了十年图书管理员。2002年，贝克韦尔辞去工作，开始专职写作，除本书外，她的作品还包括How to Live（2010）、The English Dane（2005）、The Smart（2002）。她目前生活在伦敦，并在伦敦城市大学和开放大学教授创意写作课。</p>
<img src="/2021/06/30/%E5%AD%98%E5%9C%A8%E4%B8%BB%E4%B9%89%E5%92%96%E5%95%A1%E9%A6%86/2.jpg" class="">

<hr>
<h3 id="我是如何发现这本书的？"><a href="#我是如何发现这本书的？" class="headerlink" title="我是如何发现这本书的？"></a>我是如何发现这本书的？</h3><p>在高中时期曾经一度迷恋于18世纪的黑格尔笔下的德国哲学革命，以及以他自己为首的黑格尔哲学。也是从黑格尔哲学中，我开始了解到当时德国哲学变革中的“精神现象学”——即自我意识的确定性，也创立一个完整的客观唯心主义哲学体系。这在书里第二章关于胡塞尔的唯心主义倾向也有所类似的展开。</p>
<p>看到身边朋友推荐到这本书记，又被它独特的写作体裁（传记与哲学、历史相结合）所吸引，相信会给读者迸发出不一样的阅读体验。</p>
<h3 id="推荐给谁来阅读？"><a href="#推荐给谁来阅读？" class="headerlink" title="推荐给谁来阅读？"></a>推荐给谁来阅读？</h3><p>正如上文所写，独特的传记形式让读者可以身临其境地抛开哲学术语，直接从哲学家们的个人经验和视角去感受哲学历史上的挑战和怀疑。</p>
<img src="/2021/06/30/%E5%AD%98%E5%9C%A8%E4%B8%BB%E4%B9%89%E5%92%96%E5%95%A1%E9%A6%86/3.jpg" class="">

<p>整本书的结构还是以历史上不同的哲学家进行划分，如果是某一位哲学家、或是哲学流派感兴趣可以直接选择性地挑选章节来阅读。</p>
<p>这次我的读书笔记主要是基于本书的前三章来进行写作和整理。</p>
<h2 id="读后感"><a href="#读后感" class="headerlink" title="读后感"></a>读后感</h2><blockquote>
<p>生活/行为/思维/想法上的影响</p>
</blockquote>
<p><strong>现象学</strong></p>
<p>萨特、波伏娃的好友阿隆最先提出了现象学（phenomenology）这门学科，书中将它称之为“一种把哲学与日常生活经验重新联结起来研究哲学的方式”。</p>
<p>现象学家中最重要的思想家埃德蒙德·胡塞尔，提出了一个振奋的口号：“回到事物本身（to the things themselves）! ”意思是说比起不断去尝试诠释一样事物，我们需要去观察把自己呈现在你面前的“这个东西”，且不管“这个东西”可能是什么，然后尽可能精确地把它描述出来。另一个现象学家马丁·海德格尔也表达过类似的观点：纵观历史，所有哲学家都把时间浪费在了次要问题上，而忘记去问那个最重要的问题——存在（being）的问题。</p>
<p>萨特把这个原则变成了一句三个单词的口号——“存在先于本质”（Existence precedes essence）。在他看来，这个信条便足以概括存在主义。不过，它虽有简明扼要之优，可也有不易理解之劣。大概来讲，它的意思就是，发现自己被抛入世界中后，我会持续创造我自己的定义（或本性，或本质），但其他客体或生命形式却不会这样。</p>
<p><strong>获得自由与真实</strong></p>
<p>在萨特的一次演讲后的采访中他自己总结到：</p>
<blockquote>
<p>没有任何划定的道路来引导人去救赎自己；他必须不断创造自己的道路。但是，创造道路，他便拥有了自由与责任，失去了推脱的借口，而所有希望都存在于他本身之中。</p>
</blockquote>
<p>这一令人振奋的思想正好处于早已确立的社会和政治制度遭到战争破坏的1945年。萨特的听众听到他传递的信息时，正值欧洲满目疮痍，纳粹死亡集中营的消息开始暴露出来，广岛和长崎被原子弹摧毁之际。战争使人们意识到了自己和自己的那些人类同胞，完全有能力偏离文明的规范。</p>
<blockquote>
<p>哲学既不是纯粹的智识追求，也不是廉价的自我帮助技巧的集合，而是一种训练，由此来让自己不断成长，过上完满之人那种负责任的生活。</p>
</blockquote>
<p>我觉得在这里也很好地表达了作者对于哲学发展的理解，即在不断的挑战和怀疑中去萌生新的思想。而在哲学核心的背后，真正值得我们关注的，也许并非是哲学概念，而是人生。这一点在书里接下来第二章讨论的神学存在主义得到了进一步的解释。</p>
<p><strong>神学存在主义</strong></p>
<p>对于索伦·克尔凯而言，他并不同意郭尔勒内·笛卡儿（René Descartes）所阐明的“我思故我在”（Cogito ergo sum）的现代哲学理念。在他自己看来，人的存在是在先的：是我们做每一件事的起点，而不是一个逻辑推演的结果。我的存在是主动的：我经历存在、选择存在，这先于我可以做的任何关于我自己的阐述。</p>
<p>克尔凯郭尔和尼采是现代存在主义的先驱。他们开创了一种反抗和不满的情绪，创造了存在的一种新定义，那就是选择、行动和自我肯定，并对人生的痛苦和困难做了研究。而且他们还引入了一个坚定的信念：哲学不只是一份职业，而是生命本身——个人的生命。</p>
<p><strong>《第二性》</strong></p>
<p>这本书是西蒙娜·德·波伏娃出版于1949年的一份开拓性的女性主义研究。高中时候拜读过其中的部分内容，当时并没有意识到我是在读一本存在主义著作——女性努力改变自己的人生，走的正是存在主义者的那条路，也就是追求自由，追求一种高度的个人主义和“真实性”。</p>
<p>我想这也是存在主义给予人们的启发之一，去真诚而自由地活出自己的人生。</p>
<p>《存在主义咖啡馆》这本书很棒的地方在于作者没有直接从哲学角度含沙射影地去解释什么是存在主义，或是其他哲学发展的理念。而是通过对人物经历/性格的评价去彰显哲学思想的魅力和美丽。</p>
<p>书里对萨特的评价：</p>
<blockquote>
<p>萨特借鉴了很多哲学传统，并从现代和个人的角度对其重新进行了改造，可以说，他是通往所有这些传统的一座桥梁。然而，他却一辈子都坚持认为，真正重要的不是过去，而是未来。一个人必须不断前行，创造还未发生的事：走到世界中，行动起来，然后去影响它。</p>
</blockquote>
<p><strong>再看现象学</strong></p>
<p>胡塞尔式的“括除在外”或悬搁判断，让现象学家在探询一个人如何经历着他或她的世界时，暂时忽略“这是真的吗”这个问题。</p>
<p>它让医学症状被视作病患体验到的状态成为可能，而不是仅仅被视作生理过程。病人可以描述一种弥散性或突然剧烈的疼痛，抑或一种沉重或迟滞的感觉，或是在翻江倒海的胃里有说不清的不自在感。被截肢的人常常在失去肢体的部位遭受“幻肢”之苦；现象学让分析这些感觉成为可能。</p>
<p>摘用书里的一段文字来表达：</p>
<blockquote>
<p>现象学把我们生活的世界还给了我们。在那些我们通常不认为是哲学内容的事物上，它尤其有效：一杯饮料、一首忧郁的歌、一次兜风、一抹余晖、一种不安的情绪、一盒相片、一个无聊的时刻。它通过掉转我们自己通常如空气般被忽略的视角，恢复了个人世界的丰富性。</p>
</blockquote>
<p><strong>海德格尔</strong></p>
<p>乔治·斯坦纳认为，海德格尔的意图不是要被理解，而是要通过“感觉到的陌生感”被体验。这是一种类似贝尔托·布莱希特（Bertholt Brecht）在他的戏剧中运用的“异化”（alienation）或疏离的效果，目的是防止人们变得入戏太深以及对熟悉事物的错觉信以为真。</p>
<blockquote>
<p>海德格尔的语言会令你一直紧张不安，是动态、突兀的语言，有时会显得荒诞，但总是很有力；在海德格尔作品的随便一页上，事物通常被展现为是在涌动或者推搡，被展现为被扔出去、点燃或打破。海德格尔承认，他这种写作方式造成一些“尴尬”，但他认为，那是为了颠覆哲学史并把我们带回到存在而付出的小代价。</p>
</blockquote>
<p>Sich-vorweg-schon-sein-in- （der-welt） als Sein-bei （innerweltlich begebnendem Seienden），即“先于自身-已存在于世界中-同时与世界里遇到的存在者一起的存在”。不同于胡塞尔向着内心世界的存在主义，在海德格尔看来，所有的在世存在也是一种“共在”（Being-with），或曰Mitsein。我们与他人共居于一个“共同世界”（with-world），或曰Mitwelt。</p>
<p><strong>新的思考</strong></p>
<p>从这本书里我们可以感受不同哲学家对于人类存在充满生命力的诠释，我也相信不同的读者在阅读的时候会与其中一些哲学思想、或是人生故事产生不同程度上的共鸣，这也是让我觉得阅读非常大的乐趣之一。</p>
<p>感谢作者为我们和这些哲学家们建立起了共情，共同思考和体会我们的存在。</p>
<h2 id="书中摘录"><a href="#书中摘录" class="headerlink" title="书中摘录"></a>书中摘录</h2><ol>
<li>有人说，存在主义不太像哲学，倒是更像一种情绪。简言之，可以追溯到每一个曾对任何事感到过不满、叛逆和格格不入的人。</li>
<li>他（萨特）以一种现象学创立者未曾想见的但却更让人兴奋和个人化的方式，把现象学应用到人们的生活之中，创建了一种兼具国际影响和巴黎风味的新哲学：现代存在主义。</li>
<li>自由，在萨特看来，位于人类所有经验的中心，正是这一点，才把人类与其他事物区分开来。……我总是先我自己一步，边前行，边构筑自身。</li>
<li>他的脸从来就不丑，因为他的脸被他头脑的睿智、“火山爆发般的诚实”和“新开垦土地般的慷慨”点亮了。（萨特）</li>
<li>你经历了什么，那这个什么就是个哲学话题。</li>
<li>哲学既不是纯粹的智识追求，也不是廉价的自我帮助技巧的集合，而是一种训练，由此来让自己不断成长，过上完满之人那种负责任的生活。</li>
<li>这种不断的选择带来了一种深深的忧虑，很像是从悬崖往下看时的眩晕。它不是对坠崖的恐惧，而是对你不确定自己不会把自己扔下去的恐惧。</li>
<li>每一个伟大的哲学家，实际上都是在书写“一种不自觉和无意识的回忆录”，而不是在进行客观的知识研究。研究我们自己的道德谱系学，不能帮助我们摆脱或超越自身，但它能让我们更清晰地看到我们的幻觉，并过上一种更有活力、更坚定的生活。</li>
<li>他在海边散步，把鹅卵石扔进像粥一样的灰色大海中。他走到一个公园里，盯着一棵栗树暴露在外的粗糙多节的根，感觉它看上去就像煮沸的皮革，威胁着要用它那晦涩难懂的存在让他不知所措。</li>
<li>胡塞尔在概括每一个点时，“右手的手指来回在平摊的左手手掌上缓慢画圈”——仿佛正在手掌上把他的观念翻来翻去，从不同的角度去看待它。</li>
<li>因为意识没有“内在”。它只是它自身的外表，正是这种绝对的逃离，这种对成为物质的拒绝，使它成为一种意识。</li>
</ol>
]]></content>
      <categories>
        <category>book review</category>
      </categories>
      <tags>
        <tag>社会学</tag>
        <tag>哲学</tag>
        <tag>人性</tag>
      </tags>
  </entry>
  <entry>
    <title>Body Reform | 健身日志02</title>
    <url>/2021/07/05/%E5%81%A5%E8%BA%AB%E6%97%A5%E5%BF%9702/</url>
    <content><![CDATA[<blockquote>
<p>Train like a Spartan.</p>
</blockquote>
<p>4月末之后就基本以居家健身为主，力量训练都是借用哑铃来完成。</p>
<p>和4月的健身计划相比，6月我开始加大了有氧的训练量，总共完成了103km的跑量。在饮食上也适当放宽了碳水的摄入量，从而来维持同等强度的力量训练。</p>
<img src="/2021/07/05/%E5%81%A5%E8%BA%AB%E6%97%A5%E5%BF%9702/3.png" class="" title="Keep跑步">

<p>这次的日志记录了6月末为期10天的训练内容，平均下来每周会训练腹部、手臂、肩膀和背部肌群2次，腿部肌肉没有特别训练，主要通过有氧跑步时的变速跑和HIIT中的多种复合型跳跃动作来刺激肌群。</p>
<img src="/2021/07/05/%E5%81%A5%E8%BA%AB%E6%97%A5%E5%BF%9702/1.png" class="">

<img src="/2021/07/05/%E5%81%A5%E8%BA%AB%E6%97%A5%E5%BF%9702/2.png" class="">

<p>整体来看跑步时的心肺得到了不小的提升，腿部的肌肉也没有刚开始跑步那样酸痛。运动前后的热身与拉伸也花了更长的时间去完成，充分地去调动和放松训练的肌群。</p>
<p>希望7月可以继续维持现有的有氧量，适当提升下配速。在饮食上继续控制糖分的摄入，加强对于脂肪摄入的控制。</p>
<hr>
<p>运动带给我们的好处非常之多，无论背后的目标是减脂、增肌、身体塑性或是长久形成的生活习惯，都可以借由社交平台一起来探讨训练内外的各种心得和知识。之后的内容会继续从饮食、睡眠、训练、心态等等的角度来记录健身日常，喜欢的小伙伴欢迎点赞支持一下哟，一起自律打卡。</p>
]]></content>
      <categories>
        <category>hobbies</category>
      </categories>
      <tags>
        <tag>training</tag>
      </tags>
  </entry>
</search>
